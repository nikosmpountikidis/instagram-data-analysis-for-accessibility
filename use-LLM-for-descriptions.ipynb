{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* This Notebook describes how to use the trained model to check which descriptions are accessible and which are not.\n",
        "\n",
        "* At the end it computes the overall percentage of which descriptions are accessible and which not."
      ],
      "metadata": {
        "id": "Mre_ZecJUemX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Upload in Google Colab the files that were saved in the training of the model\n",
        "# and save them inside a new folder, so that you can use its path\n",
        "\n",
        "model_dir = \"path to the folder\"\n",
        "\n",
        "# Load tokenizer and model from local directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir, local_files_only=True)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded and using device: {device}\")"
      ],
      "metadata": {
        "id": "3zvIwBYCQfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction of some examples for testing\n",
        "\n",
        "text_list = [\n",
        "    \"This is an example.\",\n",
        "    \"Another input sentence goes here.\",\n",
        "    \"Example of something else.\"\n",
        "]\n",
        "max_length = 64\n",
        "batch_size = 32\n",
        "\n",
        "results = []\n",
        "for i in range(0, len(text_list), batch_size):\n",
        "    batch_texts = text_list[i:i+batch_size]\n",
        "    encoded = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        preds = torch.argmax(probs, dim=-1)\n",
        "\n",
        "    for j, text in enumerate(batch_texts):\n",
        "        results.append({\n",
        "            \"text\": text,\n",
        "            \"predicted_label\": preds[j].item(),\n",
        "            \"prob_class_0\": round(probs[j][0].item(), 4),\n",
        "            \"prob_class_1\": round(probs[j][1].item(), 4)\n",
        "        })\n",
        "\n",
        "import pandas as pd\n",
        "result_df = pd.DataFrame(results)\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "uk07AhBTQho3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction of a CSV file with data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/14. Personal-Accounts-Results.csv\"\n",
        "max_length = 64\n",
        "batch_size = 32\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "if \"description\" not in df.columns:\n",
        "    raise ValueError(\"CSV file must contain a 'description' column.\")\n",
        "texts = df[\"description\"].astype(str).tolist()\n",
        "\n",
        "results = []\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = texts[i:i+batch_size]\n",
        "    encoded = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        preds = torch.argmax(probs, dim=-1)\n",
        "\n",
        "    results.extend(preds.cpu().tolist())\n",
        "\n",
        "# Compute statistics\n",
        "\n",
        "total = len(results)\n",
        "accessible = results.count(1)\n",
        "inaccessible = results.count(0)\n",
        "\n",
        "print(f\"Total descriptions: {total}\")\n",
        "print(f\"Accessible (label 1): {accessible} ({accessible / total:.2%})\")\n",
        "print(f\"Inaccessible (label 0): {inaccessible} ({inaccessible / total:.2%})\")\n"
      ],
      "metadata": {
        "id": "Ig520sdlQlKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}