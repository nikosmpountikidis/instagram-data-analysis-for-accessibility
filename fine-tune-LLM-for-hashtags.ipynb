{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this notebook we fine-tune an LLM model so that it can recognize if the hashtags that are used in a post on the Instagram, are accessible or not for people that use a screen reader.\n",
        "\n",
        "* Hashtags that do not use PascalCase, that use slang language, that have emojis, that have weird words (e.g. #Heloooo), are difficult for a screen reader user to understand.\n",
        "\n",
        "* The model that we used is BERT from the Hugging Face. It is trained in more than 100 languages, so it can be used in many different cases."
      ],
      "metadata": {
        "id": "pkwrLbkLDbTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install -q transformers datasets scikit-learn peft bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "7KBlCJxxyJjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "5bi_GWliyJft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load & preprocess the CSV that contains that data to fine-tune the model\n",
        "# Can be used the already CSV or something that you will build on your own\n",
        "\n",
        "df = pd.read_csv('the CSV file')\n",
        "df = df.rename(columns={df.columns[0]: \"text\", df.columns[1]: \"label\"})\n",
        "df = df.dropna().sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df['label'] = df['label'].astype(int)"
      ],
      "metadata": {
        "id": "fstqpZSDyJds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the label column includes only 0 and 1, otherwise the model\n",
        "# will encounter problems\n",
        "\n",
        "df.label.unique()"
      ],
      "metadata": {
        "id": "NQb1P03TYH1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop each column that may be empty and convert the type of the values\n",
        "# in the label column to int\n",
        "\n",
        "df = df[df['label'].isin([0, 1])].dropna()\n",
        "df['label'] = df['label'].astype(int)"
      ],
      "metadata": {
        "id": "SEIR0-ybZDqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset & split\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "RY_H5lm9yI0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer & tokenization (with padding/truncation)\n",
        "\n",
        "model_checkpoint = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=32\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")"
      ],
      "metadata": {
        "id": "xKMX31DxyVNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator to pad batches and create data loaders to use them with PyTorch\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    batch_size=32,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "qhl-EBuwyVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA is used to train only a small percentage of the parameters of the\n",
        "# BERT model, because the huge amount of the parameters would need huge\n",
        "# computational power to be trained.\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=2\n",
        ")\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "94tAJcTEyVE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU if available\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "p99jXgNMyU-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "id": "5z6JguMvyU4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "model.train()\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "m8oknVbayUxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on the 20% test split\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**batch).logits\n",
        "        preds = logits.argmax(-1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {acc:.4f}   |   F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "OG9rUkoZypPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick interactive predictions\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"/content/hashtag-accessibility-model\",\n",
        "    tokenizer=\"/content/hashtag-accessibility-model\",\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "label_map = {\"LABEL_0\": \"Inaccessible\", \"LABEL_1\": \"Accessible\"}\n",
        "\n",
        "print(\"\\nğŸ” Example Predictions:\")\n",
        "for tag in [\"#Î¨Î·Ï†Î¹Î±ÎºÎ®ÎšÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î±\", \"#ÎºÎ±Î¸Î·Î¼ÎµÏÎ¹Î½ÏŒÏ„Î·Ï„Î¬Î¼Î¿Ï…\", \"#ÎˆÎ¾Ï…Ï€Î½Î·Î–Ï‰Î®\", \"#Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬Î½ÎµÎ±Î½Ï‰Î½\", \"#Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÎ®Î£ÎºÎ­ÏˆÎ·\", \\\n",
        "      \"#Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±ÏƒÎ®Î¼ÎµÏÎ±\", \"#Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ·Î¼Î±Ï„Î¹ÎºÎ­Ï‚Î™Î´Î­ÎµÏ‚\", \"#Ï€ÏÎ¬ÏƒÎ¹Î½Î·ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±\", \"#Î•Î»Î»Î·Î½Î¹ÎºÎ®ÎšÎ¿Ï…Î¶Î¯Î½Î±\", \"#Ï„Î±Î¾Î¯Î´Î¹Î±ÎµÎ»Î»Î¬Î´Î±\", \\\n",
        "      \"#ÎšÎ±Î¹Î½Î¿Ï„ÏŒÎ¼ÎµÏ‚Î›ÏÏƒÎµÎ¹Ï‚\", \"#ÎµÏ…ÎµÎ¾Î¯Î±ÎºÎ±Î¹Î¶Ï‰Î®\", \"#Î¨Î·Ï†Î¹Î±ÎºÎ¬Î•ÏÎ³Î±Î»ÎµÎ¯Î±\", \"#ÎµÏÎ³Î±ÏƒÎ¯Î±ÎµÎ¾Î±Ï€Î¿ÏƒÏ„Î¬ÏƒÎµÏ‰Ï‚\", \"#Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Î£Ï„Î¿Î”Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿\", \\\n",
        "      \"#Î¼Î±Î³ÎµÎ¹ÏÎµÏÎ¿Ï…Î¼ÎµÎ¼Î±Î¶Î¯\", \"#ÎÎ­ÎµÏ‚Î™Î´Î­ÎµÏ‚\", \"#Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¹ÎºÎ¬Î½ÎµÎ±\", \"#Î•Ï€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÎ®Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·\", \"#ÎºÎ±Î»ÏÏ„ÎµÏÎ·Î¶Ï‰Î®\", \\\n",
        "      \"#Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î·Î›ÏÏƒÎ·\", \"#Ï€ÎµÏÎ¹Î²Î±Î»Î»Î¿Î½Ï„Î¹ÎºÎ®Î´ÏÎ¬ÏƒÎ·\", \"#Î”Î¹Î±Î´Î¹ÎºÏ„Ï…Î±ÎºÎ®ÎœÎ¬Î¸Î·ÏƒÎ·\", \"#Î¿Î¼Î¿ÏÏ†Î¹Î¬Ï†Ï…ÏƒÎ¹ÎºÎ¬\", \"#Î Î¿Î»Î¹Ï„Î¹ÏƒÏ„Î¹ÎºÎ®ÎšÎ»Î·ÏÎ¿Î½Î¿Î¼Î¹Î¬\", \\\n",
        "      \"#Î¿Î¹ÎºÎ¿Î³ÎµÎ½ÎµÎ¹Î±ÎºÎ­Ï‚ÏƒÏ„Î¹Î³Î¼Î­Ï‚\", \"#Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒÎ ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿\", \"#ÎºÎ±Î¸Î·Î¼ÎµÏÎ¹Î½Î®ÎµÎ½Î­ÏÎ³ÎµÎ¹Î±\", \"#Î ÏÎ¬ÏƒÎ¹Î½ÎµÏ‚Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯ÎµÏ‚\", \"#ÎµÏ…ÎºÎ±Î¹ÏÎ¯ÎµÏ‚ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚\", \\\n",
        "      \"#Î–Î¿ÏÎ¼ÎµÎ¨Î·Ï†Î¹Î±ÎºÎ¬\", \"#Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±ÏƒÏ„Î·Î¶Ï‰Î®\", \"#Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·Î”ÎµÎ¾Î¹Î¿Ï„Î®Ï„Ï‰Î½\", \"#Ï€ÏÎ¿ÏƒÏ‰Ï€Î¹ÎºÎ®ÎµÎ¾Î­Î»Î¹Î¾Î·\", \"#Î•ÏÎ³Î±Î»ÎµÎ¯Î±ÎœÎ¬ÏÎºÎµÏ„Î¹Î½Î³Îº\", \\\n",
        "      \"#ÎºÎ¿Ï…Î»Ï„Î¿ÏÏÎ±ÎºÎ±Î¹Ï„Î­Ï‡Î½Î·\", \"#Î•Î»Î»Î¬Î´Î±2025\", \"#Ï†Î¹Î»Î¿Î¾ÎµÎ½Î¯Î±Î¼ÎµÏˆÏ…Ï‡Î®\", \"#Î¹Î´Î­ÎµÏ‚Î³Î¹Î±Ï„Î¿ÏƒÏ€Î¯Ï„Î¹\", \"#Î•Ï€Î¹Ï‡ÎµÎ¹ÏÎ·Î¼Î±Ï„Î¹ÎºÏŒÏ„Î·Ï„Î±\", \\\n",
        "      \"#ÏƒÏ„Ï…Î»Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ÎµÏ€Î¹Î»Î¿Î³Î­Ï‚\", \"#Î¨Î·Ï†Î¹Î±ÎºÎ®Î•Ï€Î¿Ï‡Î®\", \"#ÎºÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¹ÎºÎ±Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Î±\", \"#Î¥Î³ÎµÎ¯Î±ÎšÎ±Î¹Î•Ï…ÎµÎ¾Î¯Î±\", \"#Î±Î³Î¿ÏÎ¬ÎºÎ±Î¹Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±\", \\\n",
        "      \"#ÎÎµÎ±Î½Î¹ÎºÎ®ÎšÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î±\", \"#Ï€Î±Î¹Î´ÎµÎ¯Î±Ï„Î¿Ï…Î¼Î­Î»Î»Î¿Î½Ï„Î¿Ï‚\", \"#Î”Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿Î¤Ï‰Î½Î ÏÎ±Î³Î¼Î¬Ï„Ï‰Î½\", \"#Ï„Î­Ï‡Î½Î·ÏƒÏ„Î¿Ï…Ï‚Î´ÏÏŒÎ¼Î¿Ï…Ï‚\", \"#Ï€Î¿Î»Î¹Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬Î³ÎµÎ³Î¿Î½ÏŒÏ„Î±\"]:\n",
        "    res = classifier(tag)[0]\n",
        "    print(f\"{tag:25} â†’ {label_map[res['label']]} ({res['score']:.2f})\")"
      ],
      "metadata": {
        "id": "99bYs3y7xqII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After youâ€™ve run your eval loop and gathered all_labels & all_preds:\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"Inaccessible\",\"Accessible\"], output_dict=True)\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Turn into DataFrames for readability\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "cm_df     = pd.DataFrame(cm,        index=[\"True Inac\",\"True Acc\"], columns=[\"Pred Inac\",\"Pred Acc\"])\n",
        "\n",
        "print(\"Classification Report:\\n\", report_df)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm_df)\n"
      ],
      "metadata": {
        "id": "enVwLy6N1uKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load adapter config\n",
        "peft_model_path = \"Hashtag_LLM_Model\"\n",
        "config = PeftConfig.from_pretrained(peft_model_path)\n",
        "\n",
        "# Load base model and merge with adapter\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=2)\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the full merged model\n",
        "model.save_pretrained(\"merged_model\")\n",
        "tokenizer.save_pretrained(\"merged_model\")\n"
      ],
      "metadata": {
        "id": "Avt2DB6c3ebk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}