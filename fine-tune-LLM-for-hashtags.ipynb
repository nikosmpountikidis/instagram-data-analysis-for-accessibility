{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this notebook we fine-tune an LLM model so that it can recognize if the hashtags that are used in a post on the Instagram, are accessible or not for people that use a screen reader.\n",
        "\n",
        "* Hashtags that do not use PascalCase, that use slang language, that have emojis, that have weird words (e.g. #Heloooo), are difficult for a screen reader user to understand.\n",
        "\n",
        "* The model that we used is BERT from the Hugging Face. It is trained in more than 100 languages, so it can be used in many different cases."
      ],
      "metadata": {
        "id": "pkwrLbkLDbTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install -q transformers datasets scikit-learn peft bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "7KBlCJxxyJjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel, PeftConfig\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "5bi_GWliyJft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load & preprocess the CSV that contains that data to fine-tune the model\n",
        "# Can be used the already CSV or something that you will build on your own\n",
        "\n",
        "df = pd.read_csv('the CSV file')\n",
        "df = df.rename(columns={df.columns[0]: \"text\", df.columns[1]: \"label\"})\n",
        "df = df.dropna().sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df['label'] = df['label'].astype(int)"
      ],
      "metadata": {
        "id": "fstqpZSDyJds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the label column includes only 0 and 1, otherwise the model\n",
        "# will encounter problems\n",
        "\n",
        "df.label.unique()"
      ],
      "metadata": {
        "id": "NQb1P03TYH1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop each column that may be empty and convert the type of the values\n",
        "# in the label column to int\n",
        "\n",
        "df = df[df['label'].isin([0, 1])].dropna()\n",
        "df['label'] = df['label'].astype(int)"
      ],
      "metadata": {
        "id": "SEIR0-ybZDqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset & split\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "RY_H5lm9yI0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer & tokenization (with padding/truncation)\n",
        "\n",
        "model_checkpoint = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=32\n",
        "    )\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")"
      ],
      "metadata": {
        "id": "xKMX31DxyVNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator to pad batches and create data loaders to use them with PyTorch\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    tokenized_dataset[\"test\"],\n",
        "    batch_size=32,\n",
        "    collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "qhl-EBuwyVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA is used to train only a small percentage of the parameters of the\n",
        "# BERT model, because the huge amount of the parameters would need huge\n",
        "# computational power to be trained.\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=2\n",
        ")\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "94tAJcTEyVE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move to GPU if available\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "p99jXgNMyU-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "id": "5z6JguMvyU4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "model.train()\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch in loop:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "m8oknVbayUxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on the 20% test split\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = model(**batch).logits\n",
        "        preds = logits.argmax(-1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "print(f\"Test Accuracy: {acc:.4f}   |   F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "OG9rUkoZypPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick interactive predictions\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"/content/hashtag-accessibility-model\",\n",
        "    tokenizer=\"/content/hashtag-accessibility-model\",\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "label_map = {\"LABEL_0\": \"Inaccessible\", \"LABEL_1\": \"Accessible\"}\n",
        "\n",
        "print(\"\\n🔍 Example Predictions:\")\n",
        "for tag in [\"#ΨηφιακήΚαινοτομία\", \"#καθημερινότητάμου\", \"#ΈξυπνηΖωή\", \"#οικονομικάνεανων\", \"#ΔημιουργικήΣκέψη\", \\\n",
        "      \"#τεχνολογίασήμερα\", \"#ΕπιχειρηματικέςΙδέες\", \"#πράσινηενέργεια\", \"#ΕλληνικήΚουζίνα\", \"#ταξίδιαελλάδα\", \\\n",
        "      \"#ΚαινοτόμεςΛύσεις\", \"#ευεξίακαιζωή\", \"#ΨηφιακάΕργαλεία\", \"#εργασίαεξαποστάσεως\", \"#ΕκπαίδευσηΣτοΔιαδίκτυο\", \\\n",
        "      \"#μαγειρεύουμεμαζί\", \"#ΝέεςΙδέες\", \"#τεχνολογικάνεα\", \"#ΕπαγγελματικήΑνάπτυξη\", \"#καλύτερηζωή\", \\\n",
        "      \"#ΑυτόματηΛύση\", \"#περιβαλλοντικήδράση\", \"#ΔιαδικτυακήΜάθηση\", \"#ομορφιάφυσικά\", \"#ΠολιτιστικήΚληρονομιά\", \\\n",
        "      \"#οικογενειακέςστιγμές\", \"#ΔημιουργικόΠεριεχόμενο\", \"#καθημερινήενέργεια\", \"#ΠράσινεςΤεχνολογίες\", \"#ευκαιρίεςεργασίας\", \\\n",
        "      \"#ΖούμεΨηφιακά\", \"#τεχνολογίαστηζωή\", \"#ΑνάπτυξηΔεξιοτήτων\", \"#προσωπικήεξέλιξη\", \"#ΕργαλείαΜάρκετινγκ\", \\\n",
        "      \"#κουλτούρακαιτέχνη\", \"#Ελλάδα2025\", \"#φιλοξενίαμεψυχή\", \"#ιδέεςγιατοσπίτι\", \"#Επιχειρηματικότητα\", \\\n",
        "      \"#στυλιστικέςεπιλογές\", \"#ΨηφιακήΕποχή\", \"#καινοτομικαπροϊόντα\", \"#ΥγείαΚαιΕυεξία\", \"#αγοράκαιτεχνολογία\", \\\n",
        "      \"#ΝεανικήΚαινοτομία\", \"#παιδείατουμέλλοντος\", \"#ΔιαδίκτυοΤωνΠραγμάτων\", \"#τέχνηστουςδρόμους\", \"#πολιτιστικάγεγονότα\"]:\n",
        "    res = classifier(tag)[0]\n",
        "    print(f\"{tag:25} → {label_map[res['label']]} ({res['score']:.2f})\")"
      ],
      "metadata": {
        "id": "99bYs3y7xqII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After you’ve run your eval loop and gathered all_labels & all_preds:\n",
        "report = classification_report(all_labels, all_preds, target_names=[\"Inaccessible\",\"Accessible\"], output_dict=True)\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Turn into DataFrames for readability\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "cm_df     = pd.DataFrame(cm,        index=[\"True Inac\",\"True Acc\"], columns=[\"Pred Inac\",\"Pred Acc\"])\n",
        "\n",
        "print(\"Classification Report:\\n\", report_df)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm_df)\n"
      ],
      "metadata": {
        "id": "enVwLy6N1uKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load adapter config\n",
        "peft_model_path = \"Hashtag_LLM_Model\"\n",
        "config = PeftConfig.from_pretrained(peft_model_path)\n",
        "\n",
        "# Load base model and merge with adapter\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=2)\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the full merged model\n",
        "model.save_pretrained(\"merged_model\")\n",
        "tokenizer.save_pretrained(\"merged_model\")\n"
      ],
      "metadata": {
        "id": "Avt2DB6c3ebk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}