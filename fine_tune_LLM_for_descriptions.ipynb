{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this notebook we fine-tune an LLM model so that it can recognize if the descriptions that are used in a post on the Instagram, are easy to read or not for people that use a screen reader.\n",
        "\n",
        "* Descriptions that have too many emojis, fonts that are not the default one, too many hashtags, hashtags and mentions that are everywhere in the text but in the last line grouped, create problems either on how the screen reader reads them or on the way that the description is heard by blind people.\n",
        "\n",
        "* The model that we used is BERT from the Hugging Face. It is trained in more than 100 languages, so it can be used in many different cases."
      ],
      "metadata": {
        "id": "gsLvxV8zQXCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers datasets scikit-learn peft bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "XYOTtyhn2AWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    get_scheduler,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "Cb3vMuOW2CRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare CSV that includes the training data\n",
        "\n",
        "df = pd.read_csv(\"the CSV file\")\n",
        "df = df.iloc[:, :2]\n",
        "df.columns = [\"description\", \"label\"]\n",
        "df = df.dropna()\n",
        "df = df[df[\"label\"].isin([0, 1])]\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Tokenizer and checkpoint\n",
        "checkpoint = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"description\"], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# 5-Fold Cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_preds, fold_labels = [], []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[\"label\"])):\n",
        "    print(f\"\\n=== Fold {fold+1}/5 ===\")\n",
        "\n",
        "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "    tokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=[\"description\"])\n",
        "    tokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=[\"description\"])\n",
        "\n",
        "    train_loader = DataLoader(tokenized_train, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
        "    val_loader = DataLoader(tokenized_val, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        inference_mode=False,\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=[\"query\", \"key\", \"value\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # Class weights\n",
        "    class_counts = train_df[\"label\"].value_counts().to_dict()\n",
        "    total = sum(class_counts.values())\n",
        "    weights = [total / class_counts[i] for i in range(2)]\n",
        "    class_weights = torch.tensor(weights).to(device)\n",
        "    loss_fn = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    epochs = 30\n",
        "    scheduler = get_scheduler(\n",
        "        \"linear\", optimizer=optimizer, num_warmup_steps=0,\n",
        "        num_training_steps=epochs * len(train_loader)\n",
        "    )\n",
        "\n",
        "    best_f1, no_improve, patience = 0.0, 0, 3\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in loop:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                logits = model(**batch).logits\n",
        "                pred = logits.argmax(dim=-1)\n",
        "                val_preds.extend(pred.cpu().numpy())\n",
        "                val_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "        f1 = f1_score(val_labels, val_preds)\n",
        "        print(f\"Validation F1: {f1:.4f}\")\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            no_improve = 0\n",
        "            model.save_pretrained(f\"fold_{fold+1}_adapter\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # Load best adapter and evaluate on validation again\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "    peft_model = PeftModel.from_pretrained(base_model, f\"fold_{fold+1}_adapter\")\n",
        "    peft_model = peft_model.merge_and_unload().to(device)\n",
        "    peft_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = peft_model(**batch).logits\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            fold_preds.extend(preds.cpu().numpy())\n",
        "            fold_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "# Final evaluation across all folds\n",
        "acc = accuracy_score(fold_labels, fold_preds)\n",
        "f1 = f1_score(fold_labels, fold_preds)\n",
        "print(f\"\\nFinal Accuracy: {acc:.4f} | F1 Score: {f1:.4f}\")\n",
        "print(classification_report(fold_labels, fold_preds))\n",
        "\n",
        "# Save LoRA adapter\n",
        "adapter_save_path = \"lora_adapter\"\n",
        "model.save_pretrained(adapter_save_path)\n",
        "\n",
        "# Merge adapter into base model and save\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "peft_model = PeftModel.from_pretrained(base_model, adapter_save_path)\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"merged_model\")\n",
        "tokenizer.save_pretrained(\"merged_model\")"
      ],
      "metadata": {
        "id": "uRAXUtRX2FmE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}